# -*- coding: utf-8 -*-
"""ai-for-real.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/h08765905c31b41a0bc660fbb7/locations/us-east4/repositories/29f74e21-14d2-46c9-a264-6476a03efcc3


"""## Code"""

from google.cloud import storage
import os
import json
import pandas as pd
import pandas as pd
import numpy as np
from scipy.fft import rfft, rfftfreq
from typing import List, Tuple
from google import genai
from google.genai import types
import prompts
import glob
from google.oauth2 import service_account
import re


# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'h08765905c31b41a0bc660fbb7-39156ac0ce7a.json'


def get_raw_df_from_bucket(bucket_name: str='dch_bucket', folder_prefix: str='Data/') -> pd.DataFrame:
  # ----------- Set local folder details ----------
  base_path = "./Data"
  
  if not os.path.exists(base_path):
      print(f"Data folder not found at {base_path}")
      return pd.DataFrame()
  
  # ----------- Get all device folders ----------
  device_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]
  
  # ----------- List all JSON files in the folders ----------
  json_files = []
  for device in device_folders:
      device_path = os.path.join(base_path, device)
      device_json_files = glob.glob(os.path.join(device_path, "*.json"))
      json_files.extend(device_json_files)

  print(f"Found {len(json_files)} JSON files in {base_path}")

  # ----------- Read all JSON files ----------
  all_records = []

  for json_file in json_files:
      try:
          with open(json_file, 'r') as f:
              data = json.load(f)
              # If the file contains a list of records, extend; if dict, append
              if isinstance(data, list):
                  all_records.extend(data)
              else:
                  all_records.append(data)
      except Exception as e:
          print(f"Error reading {json_file}: {e}")

  # ----------- Create DataFrame ----------
  df = pd.DataFrame(all_records)
  print(f"Created raw DataFrame of shape: {df.shape}")
  return df

def get_fft_peaks(signal: List[float], sample_freq: int, n_peaks: int=10) -> List[Tuple[float, float]]:
    N = len(signal)
    if N < n_peaks: return []
    yf = np.abs(rfft(signal))
    xf = rfftfreq(N, 1 / sample_freq)
    yf = yf[1:]  # skip DC
    xf = xf[1:]
    if len(yf) == 0: return []
    idx = np.argsort(yf)[-n_peaks:]
    idx = idx[np.argsort(yf[idx])[::-1]]
    return [(float(xf[i]), float(yf[i])) for i in idx]

def rms(signal: List[float]) -> float:
    signal = np.array(signal)
    return float(np.sqrt(np.mean(signal ** 2)))

def create_feature_df(df: pd.DataFrame) -> pd.DataFrame:
  # --- 1. Assume df is loaded, e.g. from your GCS bucket, with one row per reading ---

  # --- 2. Convert timestamp to date and time columns ---
  df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')
  df['date'] = df['datetime'].dt.date.astype(str)
  df['time'] = df['datetime'].dt.time.astype(str)

  # --- 3. Expand vibration arrays for each axis ---
  df['vibration.x'] = df['vibration'].apply(lambda d: d['x'])
  df['vibration.y'] = df['vibration'].apply(lambda d: d['y'])
  df['vibration.z'] = df['vibration'].apply(lambda d: d['z'])

  # --- 4. FFT peaks extraction (top 10, for each axis) ---
  df['fft_peaks_x'] = df.apply(lambda row: get_fft_peaks(row['vibration.x'], row['vibSamplefrequency']), axis=1)
  df['fft_peaks_y'] = df.apply(lambda row: get_fft_peaks(row['vibration.y'], row['vibSamplefrequency']), axis=1)
  df['fft_peaks_z'] = df.apply(lambda row: get_fft_peaks(row['vibration.z'], row['vibSamplefrequency']), axis=1)

  # --- 5. RMS calculation for each axis + total ---
  df['rms_x'] = df['vibration.x'].apply(rms)
  df['rms_y'] = df['vibration.y'].apply(rms)
  df['rms_z'] = df['vibration.z'].apply(rms)
  df['rms_total'] = np.sqrt(df['rms_x']**2 + df['rms_y']**2 + df['rms_z']**2)

  # --- 6. Calculate approximate date and time ---
  df['am'] = df['time'] < '12:00:00'
  df = df.sort_values(by='timestamp')

  # --- 7. Add previous 3 RMS values for each axis (FIXED) ---
  def get_previous_values(series):
      """Get previous 3 values for each row using rolling window"""
      result = []
      for i in range(len(series)):
          if i == 0:
              result.append([series.iloc[0], series.iloc[0], series.iloc[0]])
          elif i == 1:
              result.append([series.iloc[0], series.iloc[0], series.iloc[i-1]])
          elif i == 2:
              result.append([series.iloc[0], series.iloc[i-2], series.iloc[i-1]])
          else:
              result.append([series.iloc[i-3], series.iloc[i-2], series.iloc[i-1]])
      return result

  df['prev_3_rms_x'] = get_previous_values(df['rms_x'])
  df['prev_3_rms_y'] = get_previous_values(df['rms_y'])
  df['prev_3_rms_z'] = get_previous_values(df['rms_z'])
  df['prev_3_rms_total'] = get_previous_values(df['rms_total'])

  # --- 8. Extract amplitudes from FFT peaks for moving averages ---
  def extract_amplitudes(peaks_list):
      """Extract amplitudes from list of (frequency, amplitude) tuples"""
      return [amp for freq, amp in peaks_list]

  df['fft_amplitudes_x'] = df['fft_peaks_x'].apply(extract_amplitudes)
  df['fft_amplitudes_y'] = df['fft_peaks_y'].apply(extract_amplitudes)
  df['fft_amplitudes_z'] = df['fft_peaks_z'].apply(extract_amplitudes)

  # --- 9. Calculate 5-day moving averages for FFT amplitudes ---
  def calculate_amplitude_moving_avg(amplitude_series, window=5):
      """Calculate 5-day moving average for each amplitude position"""
      # Convert series of lists to DataFrame where each column is an amplitude position
      max_peaks = 10  # Assuming 10 peaks as mentioned
      amplitude_df = pd.DataFrame(amplitude_series.tolist())

      # Calculate rolling mean for each column and convert back to list format
      rolling_means = amplitude_df.rolling(window=window, min_periods=1).mean()
      return rolling_means.apply(lambda row: row.tolist(), axis=1)

  df['fft_x_5day_avg'] = calculate_amplitude_moving_avg(df['fft_amplitudes_x'])
  df['fft_y_5day_avg'] = calculate_amplitude_moving_avg(df['fft_amplitudes_y'])
  df['fft_z_5day_avg'] = calculate_amplitude_moving_avg(df['fft_amplitudes_z'])

  # --- 10. Prepare the final feature DataFrame ---
  feature_cols = [
      'sensorId', 'vibSamplefrequency', 'humidity', 'temperature',
      'timestamp', 'date', 'time', 'am', 'vibrationMode',
      'vibration.x', 'vibration.y', 'vibration.z',
      'fft_peaks_x', 'fft_peaks_y', 'fft_peaks_z',
      'rms_x', 'rms_y', 'rms_z', 'rms_total',
      'prev_3_rms_x', 'prev_3_rms_y', 'prev_3_rms_z', 'prev_3_rms_total',
      'fft_x_5day_avg', 'fft_y_5day_avg', 'fft_z_5day_avg'
  ]
  return df[feature_cols].copy()

def make_user_prompt(row: pd.DataFrame) -> str:
    def combine_peaks_with_rolling_avg(peaks_list, rolling_avg_list):
        """Combine FFT peaks with their corresponding rolling averages"""
        enhanced_peaks = []

        for i, (freq, amp) in enumerate(peaks_list):
            # Get the rolling average for this peak position (default to amp if not available)
            rolling_avg = rolling_avg_list[i] if i < len(rolling_avg_list) else amp
            enhanced_peaks.append((freq, amp, rolling_avg))

        return enhanced_peaks

    return prompts.get_report_user_prompt(
        temp=row['temperature'],
        humidity=row['humidity'],
        rms_x=row['prev_3_rms_x'] + [row['rms_x']],
        rms_y=row['prev_3_rms_y'] + [row['rms_y']],
        rms_z=row['prev_3_rms_z'] + [row['rms_z']],
        rms_total=row['prev_3_rms_total'] + [row['rms_total']],
        fft_x=combine_peaks_with_rolling_avg(row['fft_peaks_x'], row['fft_x_5day_avg']),
        fft_y=combine_peaks_with_rolling_avg(row['fft_peaks_y'], row['fft_y_5day_avg']),
        fft_z=combine_peaks_with_rolling_avg(row['fft_peaks_z'], row['fft_z_5day_avg']),
    )

def run_llm_for_timestamp(feature_df: pd.DataFrame, date: str, am: bool, sys_prompt: str, project: str="h08765905c31b41a0bc660fbb7", n:int=0) -> pd.DataFrame:
    """
    Run the Gemini LLM for all sensors at a given timestamp and return a DataFrame of results.
    """
    filtered = feature_df[(feature_df['date'] == date) & (feature_df['am'] == am)]
    if filtered.empty:
        print(f"No data found for timestamp {date}")
        return pd.DataFrame()
    results = []
    c = 0
    for idx, row in filtered.iterrows():
        print(f"\n--- Running LLM for sensor {row['sensorId']} at {row['timestamp']} ---\n")
        user_prompt = make_user_prompt(row)
        llm_output = generate(sys_prompt, user_prompt, project=project)
        results.append({
            "sensorId": row["sensorId"],
            "date": row["date"],
            "time": row["time"] + (" AM" if (row["am"]) else " PM"),
            "llm_output": llm_output
        })
        if n > 0:
          c += 1
          if c > n:
            break
    results_df = pd.DataFrame(results)
    return results_df


from google.oauth2 import service_account
from google.auth.exceptions import DefaultCredentialsError

def generate(system_prompt, user_prompt, project="h08765905c31b41a0bc660fbb7"):
    try:
        # Set up credentials from service account file
        credentials_path = "h08765905c31b41a0bc660fbb7-6d2d4194ff50.json"  # Update this path
        
        if os.path.exists(credentials_path):
            credentials = service_account.Credentials.from_service_account_file(credentials_path)
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
            print("Using service account credentials")
        else:
            print("Warning: No service account key found. Please check the path.")
            return "Error: Credentials file not found"
        
        client = genai.Client(
            vertexai=True,
            project=project,
            location="us-east4",  # Try a specific region instead of "global"
        )

        model = "gemini-2.0-flash-001"
        contents = [
            types.Content(
                role="user",
                parts=[{"text": f"{system_prompt}\n\n{user_prompt}"}]
            )
        ]

        generate_content_config = types.GenerateContentConfig(
            temperature=0.3,
            top_p=0.95,
            max_output_tokens=4096,
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF")
            ],
            thinking_config=types.ThinkingConfig(thinking_budget=0),
        )

        result_text = ""
        for chunk in client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            result_text += chunk.text
        return result_text
        
    except DefaultCredentialsError:
        raise "Error: Google Cloud credentials not found. Please set up authentication."
    except Exception as e:
        raise f"Error calling Vertex AI: {str(e)}"
    

def parse_fault_analysis(prompt_text):
    """
    Parse fault analysis prompt using regex to extract key information

    Args:
        prompt_text (str): The structured diagnostic prompt text

    Returns:
        dict: JSON object with extracted fault information
    """
    import re
    
    # Initialize result dictionary with default values
    result = {
        "sensor_id": "",
        "fault_status": "Unknown",
        "primary_type": "Unknown",
        "confidence_percentage": 0
    }

    try:
        # Extract fault status from "FAULT STATUS: XXXXX"
        status_match = re.search(r'FAULT STATUS:\s*([A-Z\s]+?)(?:\n|CONFIDENCE)', prompt_text, re.IGNORECASE | re.DOTALL)
        if status_match:
            status = status_match.group(1).strip().upper()
            # Map status values to specified categories
            if "DETECTED" in status and "NOT" not in status:
                result["fault_status"] = "DETECTED"
            elif "NOT DETECTED" in status:
                result["fault_status"] = "NOT DETECTED"
            elif "UNCERTAIN" in status:
                result["fault_status"] = "UNCERTAIN"
            else:
                result["fault_status"] = status

        # Extract confidence percentage from "CONFIDENCE: XX%"
        confidence_match = re.search(r'CONFIDENCE:\s*(\d+)%', prompt_text, re.IGNORECASE)
        if confidence_match:
            result["confidence_percentage"] = int(confidence_match.group(1))

        # Extract primary fault type from "Primary Fault Type: XXXXX"
        primary_type_match = re.search(r'Primary Fault Type:\s*([^\n]+)', prompt_text, re.IGNORECASE)
        if primary_type_match:
            primary_type = primary_type_match.group(1).strip().upper()
            # Map to specified fault type categories
            if "BEARING" in primary_type:
                result["primary_type"] = "BEARING DEFECT"
            elif "GEAR" in primary_type:
                result["primary_type"] = "GEAR FAULT"
            elif "SHAFT" in primary_type and ("UNBALANCE" in primary_type or "IMBALANCE" in primary_type):
                result["primary_type"] = "SHAFT UNBALANCE"
            elif "MISALIGN" in primary_type:
                result["primary_type"] = "MISALIGNMENT"
            elif "LOOSE" in primary_type:
                result["primary_type"] = "LOOSENESS"
            elif "RESONANCE" in primary_type:
                result["primary_type"] = "RESONANCE"
            elif "COUPLING" in primary_type:
                result["primary_type"] = "COUPLING ISSUE"
            elif "SENSOR" in primary_type:
                result["primary_type"] = "SENSOR FAILURE"
            elif "NONE" in primary_type:
                result["primary_type"] = "NONE DETECTED"
            else:
                result["primary_type"] = primary_type

        # If no primary fault type found, try to extract from "Specific Defect:" line
        if result["primary_type"] == "Unknown":
            defect_match = re.search(r'Specific Defect:\s*([^,\n]+)', prompt_text, re.IGNORECASE)
            if defect_match:
                defect = defect_match.group(1).strip().upper()
                # Map common defect types to specified categories
                if "BEARING" in defect:
                    result["primary_type"] = "BEARING DEFECT"
                elif "GEAR" in defect:
                    result["primary_type"] = "GEAR FAULT"
                elif "SHAFT" in defect and ("UNBALANCE" in defect or "IMBALANCE" in defect):
                    result["primary_type"] = "SHAFT UNBALANCE"
                elif "MISALIGN" in defect:
                    result["primary_type"] = "MISALIGNMENT"
                elif "LOOSE" in defect:
                    result["primary_type"] = "LOOSENESS"
                elif "RESONANCE" in defect:
                    result["primary_type"] = "RESONANCE"
                elif "COUPLING" in defect:
                    result["primary_type"] = "COUPLING ISSUE"
                elif "SENSOR" in defect:
                    result["primary_type"] = "SENSOR FAILURE"
                else:
                    result["primary_type"] = defect

    except Exception as e:
        print(f"Error parsing fault analysis: {e}")
        # Return default values on error
        pass

    return result

def process_dataframe(df):
    """
    Process a DataFrame with sensorId, date, time, and llm_output columns

    Args:
        df (pandas.DataFrame): DataFrame with columns 'sensorId', 'date', 'time', 'llm_output'

    Returns:
        pandas.DataFrame: DataFrame with columns 'sensorId', 'fault_status', 'primary_type', 'confidence_percentage'
    """
    import re
    results = []

    for index, row in df.iterrows():
        try:
            # Parse the LLM output
            parsed_result = parse_fault_analysis(row['llm_output'])

            # Create result row with the sensorId from the input row
            result_row = {
                "sensorId": row['sensorId'],  # Use sensorId from input DataFrame
                "fault_status": parsed_result['fault_status'],
                "primary_type": parsed_result['primary_type'],
                "confidence_percentage": parsed_result['confidence_percentage']
            }

            results.append(result_row)

        except Exception as e:
            print(f"Error processing row {index}: {e}")
            # Add error entry
            error_row = {
                "sensorId": row['sensorId'] if 'sensorId' in row else "Unknown",
                "fault_status": "Error",
                "primary_type": "Error",
                "confidence_percentage": 0
            }
            results.append(error_row)

    # Convert to DataFrame
    result_df1 = pd.DataFrame(results)
    return result_df1

df = get_raw_df_from_bucket()
df = create_feature_df(df=df)

def get_time_table(date, am, df=df):
    llm_df = run_llm_for_timestamp(df, date=date, am=am, sys_prompt=prompts.report_sys_prompt, n=0)
    out = process_dataframe(llm_df)
    print(out)
    return out

def create_summary(llm_df):
    """
    Generates an executive summary based on the provided sensor reports.

    Args:
        reports: A list of 7 sensor diagnostic reports (strings).
        summary_prompt: The prompt to use for generating the executive summary.

    Returns:
        A string containing the executive summary.
    """
    reports = ''
    for row in llm_df['llm_output']:
      reports += row + '\n'
    exec_report = ''
    if reports:
      exec_report = generate(prompts.summary_sys_prompt, reports)  # Call generate for each sensor
    return exec_report


